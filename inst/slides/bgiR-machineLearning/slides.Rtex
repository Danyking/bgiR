\documentclass[UTF8]{beamer}
\usepackage{graphicx, color}
\usepackage{algorithm2e}
\usepackage{zhspacing}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=3em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

\usepackage{underscore}
\usetheme{JuanLesPins}
\usepackage{fontspec}
\setsansfont{Microsoft YaHei}

\usepackage{enumerate}

\AtBeginSection[]{
	\frame{
		\frametitle{Next}
		\tableofcontents[currentsection, subsectionstyle=show/shaded/hide]
	}
}

\title{ML4Bioinfor-R}

\subtitle{Machine Learning for Bioinformatics using R\\}

\author{Gang Chen\\ chengang@bgitechsolutions.cn}


%% begin.rcode logo, echo=FALSE, results='asis'
%
% if(nchar(secLogo) > 0){
% cat(paste('\\logo{\\includegraphics[width=2cm]{bgi-logo.png}\\includegraphics[width=1cm]{',secLogo,'}}', sep=""))
% }else{
% cat('\\logo{\\includegraphics[width=2cm]{bgi-logo.png}}')
% }
%% end.rcode

%%begin.rcode subtitle, echo=FALSE, results='asis'
%
%  cat(paste('\\date{',subtitle, '\\\\ \\today}\n'));
% 
%
%%end.rcode


\begin{document}
\begin{frame}
\titlepage
\end{frame}
\begin{frame}[t]\frametitle{Outline}
\tableofcontents[hideallsubsections]
\end{frame}


\section{Introduction}

\begin{frame}[fragile]
  \frametitle{Identification of Biomarker 1}

%% begin.rcode, echo=F, out.height='.8\\textheight', out.width='.8\\textwidth', fig.align='center'
%
% library(bgiR)
%
% data(exprData)
%  
% labels = rep(c("normal","cancer"),53)
%
% layout(matrix(1:16, ncol=4, nrow=4))
%
% par = par()
%
% par(mar=c(2,2,2,2))
%
% tmp = sapply(1:16, function(x) boxplot(exprData[x,labels=="normal"],exprData[x,labels=="cancer"], names=c("normal","cancer"), main=rownames(exprData)[x], col=c("green","red")))
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Identification of Biomarker 2}

%% begin.rcode, echo=F, out.height='.8\\textheight', out.width='.8\\textwidth', fig.align='center'
%
% library(bgiR)
%
% data(exprData)
%  
% labels = rep(c("normal","cancer"),53)
%
% layout(matrix(1:100, ncol=10, nrow=10))
%
% par = par()
%
% par(mar=c(1,1,1,1))
%
% tmp = sapply(1:100, function(x) boxplot(exprData[x,labels=="normal"],exprData[x,labels=="cancer"], names=c("normal","cancer"), main=rownames(exprData)[x], col=c("green","red")))
%
%% end.rcode
\end{frame}
\begin{frame}[fragile]
  \frametitle{Identification of Biomarker 3}

%% begin.rcode, echo=F, out.height='.8\\textheight', out.width='.8\\textwidth', fig.align='center'
%
% library(bgiR)
%
% data(exprData)
%  
% labels = rep(c("normal","cancer"),53)
%
% layout(matrix(1:400, ncol=20, nrow=20))
%
% par = par()
%
% par(mar=c(0,0,0,0))
%
% tmp = sapply(1:400, function(x) boxplot(exprData[x,labels=="normal"],exprData[x,labels=="cancer"], names=c("",""), main="", col=c("green","red")))
%
%% end.rcode
\end{frame}

\begin{frame}
  \includegraphics[height=.5\textheight]{escc.jpg}
  \includegraphics[height=.5\textheight]{esccNormal.jpg}\\
\vspace{.8cm}
\centerline{How to identify tumor from genomics perspective?}
\vspace{.8cm}
\centerline{\huge{The answer is Machine Learning.}}
\end{frame}

\subsection{What is Machine Learning?}

\begin{frame}
  \frametitle{What is Data Mining?}

  \begin{block}{Pang-Ning Tan, Introduction to Data Mining}
Data Mining is the process of automatically discovering useful information in large data repositories.
  \end{block}

  \begin{block}{Knowledge Discovery in Databases}
Data Mining is an integral part of knowledge discovery in databases(KDD).
  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Data Mining and KDD}
\centerline{\includegraphics[width=\textwidth]{dmkdd.png}}
\end{frame}


\begin{frame}
  \frametitle{Machine Learning and Bioinformatics}
\begin{columns}[c]
\begin{column}{.2\textwidth}
\small{Biological Experiments}\\
\tiny{Sequencing}\\
\tiny{Mass Spectrum}\\
\ldots
\end{column}
\begin{column}{.05\textwidth}
$\rightarrow$
\end{column}
\begin{column}{.2\textwidth}
\small{Preprocssing}\\
\tiny{base calling}\\
\tiny{alignment}\\
\tiny{variants}\\
\ldots
\end{column}
\begin{column}{.05\textwidth}
$\rightarrow$
\end{column}
\begin{column}{.25\textwidth}
Data Mining\\
\tiny{Classification}\\
\tiny{Regression}\\
\tiny{Feature Engineering}\\
\ldots
\end{column}

\begin{column}{.05\textwidth}
$\rightarrow$
\end{column}

\begin{column}{.2\textwidth}
Biological Knowledge
\end{column}
\end{columns}
\end{frame}


\begin{frame}
  \frametitle{Traditional Data Analysis}

  \begin{block}{Motivations}
\begin{itemize}
\item Scalability
\item High Dimensionality
\item Heterogeneous and Complex Data
\item Data Ownership and Distribution
\item Non-traditional analysis
\end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Data Mining Tasks}
\centerline{\includegraphics[height=.6\textheight]{dmtasks.jpg}}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Mining and Machine Learning}
\begin{block}{Machine Learning}
  Machine learning provides the technical basis of data mining.

---Data Mining: Practical Machine Learning Tools and Techniques
\end{block}
\end{frame}

\subsection{Organization of the course}

\begin{frame}
  \frametitle{Schedule}

  \begin{block}{Schedule}
\begin{itemize}
\item Introduction
\item Unsupervised Learning: Clustering
\item Supervised Learning: Classifition
\item Feature Learning: Feature Selection
\item Discussion
\end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Softwares}
  \begin{block}{Softwares}
\begin{itemize}
\item R: R is an free platform for data analysis and visuaztion.
\item Rtools is required for Winodws systems.
\item R packages: 
\begin{itemize}
\item \textbf{e1071} for SVM
\item \textbf{FSelector} for feature selection
\item \textbf{rpart} for decision tree
\item \textbf{bgiR} includes all necessary things\\
https://github.com/gangchen/bgiR
\end{itemize}
\item R IDE: Emacs + ESS, Vim + R-Plugin, RStudio or any other editor you like.
\item Operating Systems: Windows, Linux or Mac OS
\end{itemize}
  \end{block}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Software}

  \begin{block}{bgiR package}
    bgiR package is an R package developed for R related training.\\
    Website: https://github.com/gangchen/bgiR
  \end{block}


%% begin.rcode, eval=FALSE
%
% install.packages("devtools")
% library(devtools)
% install_github("bgiR", "gangchen")
%
%% end.rcode

\end{frame}


\section{Unspervised Learning}

\begin{frame}[fragile]
  \frametitle{Heat Map}

%% begin.rcode, echo=F, out.height='.7\\textheight', out.width='.6\\textwidth', fig.align='center'
%
% data(exprData)
% library(colorRamps)
% heatmap(exprData[apply(exprData, 1, sd)>0.05,], breaks=c(0, 0.9, 1, 1.1, 2), col=green2red(4), scale="none")
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Unspervised Learning: Clustering}
\begin{block}{What is clustering?}
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).

---Wikipedia  
\end{block}

\begin{block}{Applications}
\begin{itemize}
\item for understanding: biology, information retrieval, climate, business \ldots
\item for utility: summarization, compression, efficiently finding nearest neighbors \ldots
\end{itemize}  
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Common Clustering Methods}
\begin{block}{Clustering Methods}
  \begin{itemize}
  \item Density-based clustering
  \item K-means
  \item Hierarchical Clustering
  \item Semi-supervised clustering
  \item \ldots \ldots
  \end{itemize}
\end{block}
\end{frame}

\subsection{Unspervised Learning in Bioinformatics}

\begin{frame}[fragile]
  \frametitle{Applications of Clustering}

\begin{block}{Applications}
  \begin{itemize}
  \item Overview of dataset
  \item Discovery of unknown subtypes of diseases
  \item Relationship among features and individuals
  \item \ldots \ldots
  \end{itemize}
\end{block}

\end{frame}

\subsection{Hierarchical Clustering and its Applications in Bioinformatics}

\begin{frame}[fragile]
  \frametitle{Hierarchical Clustering}
\begin{block}{Steps}
  \begin{itemize}
  \item Calculating distance between individuals
  \item Combine closest individuals (optional, recaculate distance)
  \item Visualization and annotation
  \end{itemize}
\end{block} 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Hierarchical Clustering in R}

%% begin.rcode, eval=FALSE
% help(dist)
% help(hclust)
% help(heatmap)
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Distance Calculation}

%% begin.rcode, eval=F, tidy=F
%
% dist(x, method = "euclidean", 
% diag = FALSE, 
% upper = FALSE, 
% p = 2)
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Distance Calculation}
\tiny
%% begin.rcode
%
% exprDist = dist(t(exprData)[1:8])
% exprDist
%
%% end.rcode
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Clustering}

%% begin.rcode clusting, eval=F
%
% hclust(d, method = "complete", members = NULL)
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Clustering}
%% begin.rcode
% exprDist = dist(t(exprData))
% exprClust = hclust(exprDist)
% exprClust
%% end.rcode
\end{frame}


\begin{frame}[fragile]
  \frametitle{Visualization}
\begin{block}{Visualization}
  \begin{itemize}
  \item Dendrogram: plot.hclust
  \item Heat Map: heatmap
  \end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dendrogram}

%% begin.rcode, out.height='.7\\textheight', out.width='.9\\textwidth', fig.align='center'
%
% plot(exprClust)
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{plot.hclust}
%% begin.rcode, tidy=F, eval=F
% plot(x, labels = NULL, 
% hang = 0.1,
% axes = TRUE, 
% frame.plot = FALSE, 
% ann = TRUE,
% main = "Cluster Dendrogram",
% sub = NULL, 
% xlab = NULL, ylab = "Height", ...)
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Heat Map}
  
%% begin.rcode, out.height='.7\\textheight', out.width='.6\\textwidth', fig.align='center'
%
% heatmap(exprData)
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{heatmap}
\small
%% begin.rcode, tidy=F, eval=F
% heatmap(x, Rowv = NULL, Colv = if(symm)"Rowv" else NULL,
% distfun = dist, hclustfun = hclust,
% reorderfun = function(d, w) reorder(d, w),
% add.expr, symm = FALSE, revC = identical(Colv, "Rowv"),
% scale = c("row", "column", "none"), na.rm = TRUE,
% margins = c(5, 5), ColSideColors, RowSideColors,
% cexRow = 0.2 + 1/log10(nr), cexCol = 0.2 + 1/log10(nc),
% labRow = NULL, labCol = NULL, main = NULL,
% xlab = NULL, ylab = NULL,
% keep.dendro = FALSE, verbose = getOption("verbose"), ...)
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{How to read Heat Map}

%% begin.rcode, out.height='.8\\textheight', out.width='.7\\textwidth', fig.align='center', echo=F
%
% data(exprData)
% heatmap(exprData[apply(exprData, 1, sd)>0.05,], breaks=c(0, 0.9, 1, 1.1, 2), col=green2red(4), scale="none")
%
%% end.rcode
\end{frame}


\subsection{Summary}

\begin{frame}[fragile]
  \frametitle{Suumary}
\begin{block}{Clustering}
  \begin{itemize}
  \item Clustering is widely used in bioinformatics
  \item Clustering can be implemented by using built-in functions of R
  \item Clustering can be visualized as heat map and dendogram
  \end{itemize}
\end{block}
\end{frame}


\section{Supervised Learning}

\begin{frame}
\frametitle{Supervised Learning: Classification}

\begin{block}{Classification}
Assigning objects to one of several predefined categoriies.
\end{block}


\begin{block}{Definition}
Classification is the task of learning a \textbf{target function} $f$ that maps each attribute set $x$ to one of the predefined class labels $y$.
\end{block}
\end{frame}


\begin{frame}
  \frametitle{How to solve a classification problem?}
\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (model) {Model};
    \node [block, above left of=model, node distance=2.2cm] (lmodel) {Learn Model};
    \node [block, above of=lmodel, node distance=2cm] (lalg) {Learning Algorithm};
    \node [block, left of=lmodel, node distance=3cm] (train) {Training Set};
    \node [block, below left of=model, node distance=2.2cm] (amodel) {Apply Model};
    \node [block, left of=amodel, node distance=3cm] (test) {Testing Set};


    % Draw edges
    \path [line] (train) -- (lmodel);
    \path [line] (lalg) -- (lmodel);
    \path [line] (lmodel) -- (model);
    \path [line] (model) -- (amodel);
    \path [line] (amodel) -- (test);
\end{tikzpicture}\\
General approach for building a classification model
\end{center}
\end{frame}


\begin{frame}
  \frametitle{Evaluation}
\begin{center}
Confusion Matrix for a 2-class problem
  \begin{tabular}{|c|c|c|}
\hline
  & Prediction=1 & Prediction=0 \\
\hline
Class=1 & $f_{11}$ & $f_{10}$ \\
\hline
Class=0 & $f_{01}$ & $f_{00}$\\
\hline
  \end{tabular}
\end{center}
\end{frame}


\begin{frame}
  \frametitle{Evaluation}

  \begin{block}{Accuracy and Error Rate}

    \begin{align*}
      Accuracy & = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}\\
&=\frac{f_{11}+f_{00}}{f_{11}+f_{10}+f_{00}+f_{01}}\\
      Error Rate &= \frac{\text{Number of wrong predictions}}{\text{Total number of predictions}}\\
&=\frac{f_{10}+f_{01}}{f_{11}+f_{10}+f_{00}+f_{01}}
    \end{align*}
  \end{block}
\end{frame}

\subsection{Supervised Learning in Bioinformatics}


\begin{frame}
  \frametitle{Classification in Bioinformatics}

  \begin{block}{Applications}
\begin{itemize}
\item Classification of diseases, expecially cancer.
\item Prediction of clinical outcome.
\item Prediction of the function of gene or proteins.
\item Prediction of the structure of proteins.
\item \ldots \ldots
\end{itemize}
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Objective}
  \begin{block}{Two Objectives}
    \begin{enumerate}
      \item To build accuate classifiers or predictors
      \item To derive inferences from the results obtained
    \end{enumerate}
  \end{block}

  \begin{block}{Challanges}
    \begin{itemize}
      \item data incocnsistency and missing values
      \item noise
      \item normalization
      \item Deimensionality reduction
    \end{itemize}
  \end{block}
\end{frame}

\subsection{Decision Tree and its Applications in Bioinformatics}


\begin{frame}
  \frametitle{Decision Tree}
\centerline{\includegraphics[height=.7\textheight]{dt1.jpg}}
\end{frame}


\begin{frame}
  \frametitle{Concepts}
  \begin{block}{Types of Nodes}
    \begin{description}
      \item[root node]:no incoming edges and zero or more outgoing edges.
      \item[internal nodes]: has exactly one incoming edge and two or more outgoing edges.
      \item[leaf or terminal nodes]: has exactly one incoming edge and no outgoing edges.
    \end{description}
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{How to build a Decision Tree?}
\footnotesize
  \begin{block}{Hunt's Algorithm}
Let $D_t$ be the set of training records that are associated with node $t$ and $y={y_1, y_2,\ldots, y_c}$ to be the class labels.
    \begin{enumerate}
      \item if all the records in $D_t$ belong to the same class $y_t$, then $t$ is a leaf node labeled as $y_i$.
      \item if $D_i$ contains records that belong to more than one class, an \textbf{attribute test condition} is selected to partition the records into smaller subsets. A child node is created for each outcome of the test condition and the records in $D_t$ are distributed to the chirldren based on the outcomes. The algorithm is then recursively applied to each child node.
    \end{enumerate}
  \end{block}
\end{frame}


\begin{frame}
\footnotesize
\textbf{Algorithm} A skeleton decision tree induction algorithm\\
\textbf{TreeGrowth$(E,F)$}

\begin{algorithm}[H]
\SetAlgoLined
  \eIf{stopping_cond($E, F$) = $true$}{
       $leaf = createNode()$\;
       $leaf.label = Classify(E)$\;
       \Return{return $leaf$}
}{
       $root$ = createNode()\;
       $roor.test\_cond$ = find_best_split($E, F$)\;
       let $V = {v|v \text{is a possible outcome of }root.test\_cond}$
       \For{each $v \in V$ do}{
           $E_v = {e | root.test\_cond(e)=v \text{and} e \in E}$\;
           $child = TreeGrowth(E_v, F)$\;
           add $child$ as descendent of $root$ and label the edge ($root \rightarrow child$) as $v$}
  }
  \Return{root}
\end{algorithm}

\end{frame}


\begin{frame}
  \frametitle{Overfitting}
  \begin{block}{Reasons}
\begin{itemize}
\item Noise
\item Lack of Representative samples
\end{itemize}
  \end{block}

  \begin{block}{Occam's Razor}
Given two models with the same generalization errors, the simpler model is prefered over the more complex model.
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Handling Overfitting in Decision Tree}

  \begin{block}{Prepruning, Early Stopping Rule}
Halt tree-growing algorithm before generating a fully grown tree:
\begin{itemize}
\item the observed gain falls below a certain threshold.
\item the number of records in a node is less than a certain threshold.
\end{itemize}
  \end{block}


  \begin{block}{Post-pruning}
The decision tree is initially grown to its maximum size, then perform pruning.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Decision Tree in R}

%% begin.rcode, eval=FALSE
% library(rpart)
% library(C50)
% library(party)
%% end.rcode
\end{frame}


\begin{frame}[fragile]
  \frametitle{C5.0 Algorithm in R}

%% begin.rcode, eval=FALSE
%
% library(C50)
% C5.0(x, y)
% # x is a data frame or a matrix of records
%
% # y is a factor vector of class lables
%
%% end.rcode
\end{frame}


\begin{frame}[fragile]
\frametitle{C5.0Control}
%% begin.rcode, eval=FALSE, tidy=FALSE
%
%   C5.0Control(subset = TRUE, 
%             bands = 0, 
%             winnow = FALSE, 
%             noGlobalPruning = FALSE, 
%             CF = 0.25, 
%             minCases = 2, 
%             fuzzyThreshold = FALSE, 
%             sample = 0, 
%             seed = sample.int(4096, size = 1) - 1L,  
%             earlyStopping = TRUE,
%             label = "outcome")
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Decision Tree on Gene Expression Data}

%% begin.rcode
%
%# load BGI package for R training
% library(bgiR) 
% library(C50) # load C50 package
%# load ESCC microarray data of 1000 probesets
% data(exprData) 
% dt = C5.0(t(exprData), as.factor(rep(0:1,53)))
%
%% end.rcode
\end{frame}


\begin{frame}[fragile]
\tiny
%% begin.rcode
%
% summary(dt)
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{Performance}
%% begin.rcode
%
% pre = predict(dt, t(exprData))
% table(pre, rep(0:1, 53))
%
%% end.rcode
\end{frame}

\subsection{Support Vector Machine and its Applications in Bioinformatics}


\begin{frame}
  \frametitle{SVM}
\centerline{\includegraphics[height=.7\textheight]{svm.png}}
\end{frame}

\begin{frame}
  \frametitle{Kernal}
\centerline{\includegraphics[height=.7\textheight]{kernel.jpg}}
\end{frame}


\begin{frame}
  \frametitle{SVM in R}

%% begin.rcode, eval=FALSE
%
% library(e1071)
%
%% end.rcode

\end{frame}

\begin{frame}[fragile]
  \frametitle{SVM}

%% begin.rcode svmtraining
%
% model = svm(x = t(exprData), y = as.factor(rep(0:1, 53)))
%
%% end.rcode
\end{frame}

\begin{frame}[fragile]
  \frametitle{SVM}
%% begin.rcode svmpredict
%
% ret = predict(model, t(exprData))
% table(ret, rep(0:1, 53))
%
%% end.rcode
\end{frame}

\subsection{Summary}
\begin{frame}[fragile]
  \frametitle{Summary}
\begin{block}{Classification}

  \begin{itemize}
  \item Classification is an important technique for bioinformatics
  \item Decision tree is a rule-based classification method
  \item SVM is powerful
  \item Classification algorithm can be implemented in R easily
  \end{itemize}
\end{block}
\end{frame}

\section{Feature Learning}

\subsection{Feature Learning and Biomarker Identification}

\begin{frame}[fragile]
 
%% begin.rcode, echo=F, out.height='.9\\textheight', out.width='.9\\textwidth', fig.align='center'
%
% library(bgiR)
%
% data(exprData)
%  
% labels = rep(c("normal","cancer"),53)
%
% layout(matrix(1:400, ncol=20, nrow=20))
%
% par = par()
%
% par(mar=c(0,0,0,0))
%
% tmp = sapply(1:400, function(x) boxplot(exprData[x,labels=="normal"],exprData[x,labels=="cancer"], names=c("",""), main="", col=c("green","red")))
%
%% end.rcode
  
\end{frame}

\subsection{Feature Selection: Filter and Wrapper}


\begin{frame}[fragile]
  \frametitle{Feature Selection}
\begin{block}{Feature Selection}
Feature selection is the process of selecting a subset of relevant features for use in model construction. 
\end{block}

\begin{block}{Why?}
   Data always contains many redundant or irrelevant features.
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feature Selection}
\begin{block}{Filter and Wrapper}
\begin{itemize}
\item Filter: Chi-Squared, correlation, \ldots
\item Wrapper: Decision tree, SVM, Random Forest\ldots
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
  \frametitle{Feature Selection}

%% begin.rcode, eval=FALSE
%
% library(FSelector)
%
%% end.rcode
\end{frame}


\subsection{Filters}
\begin{frame}[fragile]
  \frametitle{Filters}
\begin{block}{Filters}
\begin{itemize}
\item Chi-squared: discrete attributes
\item Consistency: continous and discrete attributes
\item Correlation: continous attributes and continous classes
\item Entropy: discrete attributes and continous classes
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Consistency Fileter}

%% begin.rcode consistency, eval=F
%
% consistency(formula, data)
%
%% end.rcode
\end{frame}



\subsection{Summary}


\section{Future}

\begin{frame}
  \frametitle{Future Reading}
\begin{itemize}
\item Data Mining for Bioinformatics
\item Introduction to Machine Learning
\item CRAN Task View: Machine Learning \& Statistical Learning: http://cran.r-project.org/web/views/MachineLearning.html
\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Advanced Courses}
\begin{block}{Advaced Courses}
\begin{itemize}
\item Machine learning for Big Biological Data
\item Implementing high performance machine learning algorithms
\item Deep learning for bioinformatics
\item Data integration
\item Network based machine learning
\item \ldots
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
  \centerline{\Huge{Thanks!}}
\end{frame}

\end{document}